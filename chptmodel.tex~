\documentclass[11pt]{article}
\usepackage[sort,longnamesfirst]{natbib}
\usepackage{psfig}
\usepackage{amsmath}%
\newcommand{\bthet}{ \mbox{\boldmath $\theta$}}
\newcommand{\bOne}{ {\bf 1} }
\newcommand{\lambdaT}{\lambda_{\theta}}
\newcommand{\bTheta}{ \mbox{\boldmath $\Theta$}}
\begin{document}
\pagestyle{empty}
\begin{center}
\Large
%{\bf Penn State Summer School in Astrostatistics}\\
{\bf Penn State Astrostatistics MCMC tutorial}\\
\large
Murali Haran, Penn State Dept. of Statistics \\\vspace{0.2in}
{\bf  \underline{A Bayesian change point model}}\\
\end{center}
% Suggestion: please start this homework as early as possible as it is
% likely to be more time consuming than your previous assignments.\\
\normalsize
Consider the following hierarchical changepoint model for the number
of occurrences $Y_i$ of some event during time interval $i$ with change point $k$.
\begin{equation*}
\begin{split}
Y_i |k,\theta,\lambda \sim & \mbox{Poisson}(\theta) \mbox{ for } i=1,\dots,k\\
Y_i |k,\theta,\lambda \sim & \mbox{Poisson}(\lambda) \mbox{ for } i=k+1,\dots,n
\end{split}
\end{equation*}
Assume the following prior distributions:
\begin{equation*}
\begin{split}
  \theta|b_1 \sim & \mbox{Gamma}(0.5,b_1)\:\:\:\:\:\:\:\:\:\:\mbox{(pdf=} g_1(\theta|b_1))\\
  \lambda|b_2 \sim &  \mbox{Gamma}(0.5,b_2)\:\:\:\:\:\:\:\:\: \mbox{(pdf=} g_2(\lambda|b_2))\\
  b_1 \sim & \mbox{IG}(0,1)\:\:\:\:\:\:\:\:\: \mbox{(pdf=} h_1(b_1))\\
  b_2 \sim & \mbox{IG}(0,1)\:\:\:\:\:\:\:\:\: \mbox{(pdf=} h_2(b_2))\\
  k \sim & \mbox{Uniform}(1,\dots,n)\:\:\:\:\:\:\:\:\: \mbox{(pmf =} u(k))
\end{split}
\end{equation*}

$k,\theta,\lambda$ are conditionally independent and $b_1,b_2$ are independent. \\ Assume the Gamma density parameterization Gamma$(\alpha,\beta) = \frac{1}{\Gamma(\alpha)\beta^{\alpha}} x^{\alpha-1} e^{-x/\beta}$ and IG (Inverse Gamma) density parameterization IG$(\alpha,\beta) = \frac{e^{-1/\beta x}} {\Gamma(\alpha) \beta^{\alpha} x^{\alpha + 1}}$\\\\
Inference for this model is therefore based on the 5-dimensional {\bf
  posterior} distribution $f(k,\theta,\lambda,b_1,b_2| {\bf Y})$ where
{\bf Y}=$(Y_1,\dots,Y_n)$. The posterior distribution is obtained {\it
  up to a constant} (that is, the normalizing constant is unknown) by
taking the product of all the conditional distributions. Thus we have
\begin{equation*}
\begin{split}
f(k,\theta,\lambda,b_1,b_2| {\bf Y}) & \propto \prod_{i=1}^k f_1(Y_i|\theta,\lambda,k) \prod_{i=k+1}^n f_2(Y_i|\theta,\lambda,k)\\
& \times g_1(\theta|b_1) g_2(\lambda|b_2) h_1(b_1) h_2(b_2) u(k)\\
& = \prod_{i=1}^k \frac{\theta^{Y_i} e^{-\theta}}{Y_i !} \prod_{i=k+1}^n \frac{\lambda^{Y_i} e^{-\lambda}}{Y_i !} \\
& \times  \frac{1}{\Gamma(0.5)b_1^{0.5}} \theta^{-0.5} e^{-\theta/b_1} \times \frac{1}{\Gamma(0.5)b_2^{0.5}} \lambda^{-0.5} e^{-\lambda/b_2}\\ 
& \times \frac{e^{-1/b_1}}{b_1} \frac{e^{-1/b_2}}{b_2}  \frac{1}{n}
\end{split}
\end{equation*}
% Inference is based on properties of this distribution which can be
% estimated by Monte Carlo. For example: If we want to estimate the
% expected value of $\lambda$, $E_f(\lambda|{\bf Y})$, we can turn to
% Monte Carlo. All we need to do is draw samples from the distribution
% $f$. For this we turn to the Metropolis-Hastings
% algorithm or {\it Markov chain} Monte Carlo. \\\\
% Our estimate of $E_f(\lambda|{\bf Y})$ is obtained by averaging all
% $\lambda$ values in the Markov chain produced by the
% Metropolis-Hastings algorithm. More generally, we can obtain any
% expectation with respect to the posterior distribution $f$. We can
% therefore also produce estimates of the entire density for one or more
% of the random variables by using the Markov chain values.

\end{document}
