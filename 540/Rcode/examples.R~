##### August, 2016
############################################################
## Illustrate cost of multivariate normal generation
## as a function of the matrix size
## (focus on choleski)
############################################################
library(MASS)

N = 2000 #10 #2000 # or 10, 50, 100, 1000
randmean = rnorm(N)
randmat = matrix(rnorm(N*N), N,N)
covmat = randmat%*%t(randmat)

for (N in c(100,seq(1000,5000,by=1000)))
  {
    randmean = rnorm(N)
    randmat = matrix(rnorm(N*N), N,N)
    covmat = randmat%*%t(randmat)
    cat("for N=",N,"\n")
    print(system.time((chol(covmat))))
  }
##system.time((foo = mvrnorm(n=1, mu=randmean, Sigma=covmat)))

############################################################
##### Illustrate numerical instability of matrix inversion
############################################################
A = matrix(c(1.6,0,0,1.4/10), 2,2)
Ainv = solve(A)
Ainv%*%A

B = matrix(c(1.6*10^7,0,0,1.4*10^(-7)), 2,2)
Binv = solve(B)
Binv%*%B

B = matrix(c(1.6*10^8,0,0,1.4*10^(-8)), 2,2)
Binv = solve(B)
Binv%*%B

C = matrix(c(1.6*10^308,0,0,1.4*10^(-308)), 2,2)
Cinv = solve(C)
Cinv%*%C

## Illustrate the importance of memory allocation
## versus dynamic memory allocation (dynamic=allocate
## new memory as needed rather than all in advance)

N = 100000 # 10000000 #1000000
M = 50 # 500
A = matrix(rnorm(N*M), N, M)

## compute row sums by looping over the matrix rows
computeRowSum1 = function(mat)
  {
    rowsumvec = c() # initialize vector
    for (i in 1:N)
      rowsumvec = c(rowsumvec, sum(mat[i,])) # new row sum goes into vector
    return(rowsumvec)
  }
system.time((myrowsum = computeRowSum1(A)))

## ABOVE: memory for the vector to hold row sums (rowsumvec)
## is being allocated dynamically

computeRowSum2 = function(mat)
  {
    rowsumvec = rep(NA, N) # create a vector of length N (number of rows)
    for (i in 1:N)
      rowsumvec[i] = sum(mat[i,])
    return(rowsumvec)
  }

## ABOVE: now memory is allocated ahead of time. Much faster!
## is being allocated dynamically

system.time((myrowsum2 = computeRowSum2(A)))
system.time((myrowsum3 = apply(A,1, sum))) # avoid looping can also help speed things up


############################################################
## Condition number (sensitivity to changes in input
## when solving linear system)
## This is a property of the matrix, not the algorithm or
## precision of the computer!!
## But precision can impact results if condition number is
## large
############################################################
## example from R.S.Wilson (via Lange's book)
A=rbind(c(10,7,8,7), c(7,5,6,5), c(8,6,10,9), c(7,5,9,10))
Ainv = solve(A)

## perturb A
A2 = A + 0.01*diag(rep(1,4))
A2inv=solve(A2)

#########################
##### 
#########################


##################################
### an example of using apply
##################################
N = 1000000
myxs = rnorm(N)
thresh = 0.3
bar = function(xs)
{
  for (i in 1:N)
    if (xs[i]>thresh)
      xs[i] = thresh
  return(xs)
}

quux = function(xs,thresh)
{
  threshfun = function(x)
    {
      if (x>thresh)
        return(thresh)
      else
        return(x)
    }
  return(sapply(xs, threshfun))
}

system.time(xsnew = bar(myxs))
system.time(xsnew2 = quux(myxs,0.3))

#########################
## example for profiling R code
#########################

## function that generates a multivariate normal
## with covariance matrix = M*M^t where M is a matrix of N normal(0,1)
library(MASS)
generateMN = function(N)
  {
    randmean = rnorm(N)
    randmat = matrix(rnorm(N*N), N,N)
    covmat = randmat%*%t(randmat)
    X = mvrnorm(n=1, mu=randmean, Sigma=covmat)
    return(X)
  }
Rprof("profile1.out") # start profiling
foo = generateMN(1000)
Rprof(NULL) # stop profiling
##system.time(foo = mvrnorm(n=1, mu=randmean, Sigma=covmat))
summaryRprof(filename="Rprof.out")

##################################################
#### eigen decomposition
##################################################
N = 5
randmean = rnorm(N)
randmat = matrix(rnorm(N*N), N,N)
covmat = randmat%*%t(randmat)
eigenmat = eigen(covmat)
eigenmat$vectors%*%diag(eigenmat$values)%*%t(eigenmat$vectors)

eigenmatvaluesRED=c(eigenmat$values[1:3],0,0)
covmatRED =eigenmat$vectors%*%diag(eigenmatvaluesRED)%*%t(eigenmat$vectors)

##################################################
## SVD
##################################################
A = matrix(rnorm(6*4,mean=3,sd=1), 6,4) # random matrix A
svdA=svd(A) # SVD of A
A2=svdA$u%*%diag(svdA$d)%*%t(svdA$v) # this product reproduces A=UDV^t
norm(A-A2,type="F") # compute Frobenius norm
    
## now reduce dimensions: instead of all 4, use only first 3 singular values
## not a good approximation: not enough structure!
singvalRED=svdA$d
#singvalRED[3]=0
singvalRED[4]=0 # zero out 4th singular value
A3=svdA$u%*%diag(singvalRED)%*%t(svdA$v) # this product reproduces A=UDV^t
norm(A-A3,type="F") # compute Frobenius norm


#########################
## generalized inverses
#########################
library(MASS)
B=matrix(c(2,1,4,2),2,2)
solve(B) # singular
ginv(B) # but can obtain generalized inverse
## get generalized inverse using SVD
svdB=svd(B)
gRec=function(x) # generalized reciprocal
    {
        invX=rep(NA,length(x))
        for (i in 1:length(x))
            if (x[i]>0)
                invX[i]=1/x[i]
            else
                invX[i]=0
        return(invX)
    }
Dginv = gRec(svdB$d)
## pay attention to very small values in D above,
## this results in very large values in Dginv!!
Dginv[2]=0
svdB$v%*%diag(Dginv)%*%t(svdB$u)


#########################
## Example for integrate
#########################

########################################
## badly behaved function
########################################
myfn=function(x)
  return(exp(x)/sqrt(x))

integrate(myfn,0,1)
## 2.925303 with absolute error < 9.4e-06

########################################
# Toy example for numerical integration
# Conditional pdf of mu
########################################

xConst = 0.7
alphaConst = 2
betaConst = 2
  
muCondtl = function(mu)
{
  val1 = exp(-0.5*(xConst-mu)^2)/sqrt(2*pi)
  val2 = (mu^(alphaConst-1)*(1-mu)^(betaConst-1))/beta(alphaConst,betaConst)

  return(val1*val2)
}


muPrior = function(mu)
  {
    val = (mu^(alphaConst-1)*(1-mu)^(betaConst-1))/beta(alphaConst,betaConst)
    return(val)
  }
#muvals = seq(0,1,length=100)
#plot(muvals, sapply(muvals, muCondtl))
#lines(muvals, sapply(muvals, muPrior), col="red")

########################################
## calculate normalizing constant
## for above function
########################################

########################################
## Riemann integration
########################################
approxList = rep(NA,50)
for (i in 1:50)
  {
    n = 10*i # number of intervals
    nodesRie = seq(0,1,length=n)  ## divide up interval into n pieces
    muCondtlEval= sapply(nodesRie, muCondtl)
    approxInt = sum(muCondtlEval)/n
    approxList[i] = approxInt
  }
plot(approxList, ylim=c(0.34,0.4))
abline(h=0.3818935)

## Using R's quadrature function
integrate(muCondtl, 0, 1)
#0.3818935 with absolute error < 4.2e-15
## So approximation to normalizing constant is 1/0.3818935

muCondtlNormalized = function(mu)
  {
    return(muCondtl(mu)/0.3818935)
  }
integrate(muCondtlNormalized,0.5,1)
## 0.5184336 with absolute error < 5.8e-15

###############################################
## Approximating the sampling distribution of 
## skew (asymmetry) and kurtosis ("peakiness")
## skew = third central moment/standard dev^3
## kurtosis = fourth central moment/sd^4
## E(skew) = 0, E(kurtosis)=4
###############################################
M = 1000000 # Monte Carlo sample size
N = 100 # size of N(0,1)

sampleskew=rep(NA, M)
samplekurt=rep(NA, M)
for (i in 1:M)
{
  normvars = rnorm(N, 0,1)
  samplemean=mean(normvars)
  samplesd=sd(normvars)
  sampleskew[i]=mean((normvars-samplemean)^3)/(samplesd^3)
  samplekurt[i]=mean((normvars-samplemean)^4)/(samplesd^4)
}

hist(sampleskew, main=paste("Sample skew for ",N," N(0,1)"))
abline(v=0.6,col="red")
sum(sampleskew>0.6)/M

hist(samplekurt, main=paste("Sample kurtosis for ",N," N(0,1)"))
abline(v=0.5,col="red")
sum(samplekurt>0.5)/M

###############################################
## Approximating an expectation
###############################################


######################################################################
### function to plot estimate of expected value (versus sample size)
### and estimate of MCse (versus sample size)
######################################################################
## Note: can replace below with cumsum function
cummean = function(vec)
{
  cummean = rep(NA, length(vec))
  cummean[1]=vec[1]
  for (i in 2:M)
    cummean[i] = (i-1)/i*cummean[i-1] + vec[i]/i
  return(cummean)
}

cummcse = function(vec)
{
  MINLEN=50
  veclen=length(vec)
  if (veclen<=MINLEN)
    stop("Vector not long enough, should be greater than",MINLEN,"\n")
  cummcse = rep(NA, veclen-MINLEN)
  cummcse[1]=sd(vec[1:MINLEN])/sqrt(MINLEN)#
  k=1
  for (i in (MINLEN+1):veclen)
    {
      k = k+1
      cummcse[k] = sd(vec[1:i])/i
    }
  return(cummcse)
}

plotcum=function(vec)
  {
    par(mfrow=c(2,1))
    cumvec = cummean(vec)
    mcsevec = cummcse(vec)
    plot(seq(1,M), cumvec, main="Estimate of expectation versus sample size")
    plot(mcsevec, main="Estimate of standard error versus sample size")
    
    par(mfrow=c(1,1))
  }

M = 1000 # Monte Carlo sample size
######################################################
##### from above, expectation of sample kurtosis
##### for logNormal(0,1) with N=15 samples
######################################################
N=15
samplekurt=rep(NA, M)
for (i in 1:M)
{
  lognormvars = exp(rnorm(N, 0,1))
  samplemean=mean(lognormvars)
  samplesd=sd(lognormvars)
##  sampleskew[i]=mean((lognormvars-samplemean)^3)/(samplesd^3)
  samplekurt[i]=mean((lognormvars-samplemean)^4)/(samplesd^4)
}
plotcum(samplekurt)

#### expectation of a t-5 r.v.
tvars = rt(M, 5)
plotcum(tvars)
## tcum = cummean(tvars)
## tmcse = cummcse(tvars)
## par(mfrow=c(2,1))
## plot(seq(1,M), tcum, main="Estimate of expectation versus sample size")
## plot(tmcse, main="Estimate of standard error versus sample size")

#### expectation of a Cauchy does not exist!!
cauchyvars = rcauchy(M, 0,1)
plotcum(cauchyvars)

