\documentclass[12pt]{article}
\usepackage[sort,longnamesfirst]{natbib}
\usepackage{amsmath}%
\newcommand{\suc}{\mbox{ succeeds }}
\newcommand{\fai}{\mbox{ fails }}
\begin{document}
%\pagestyle{empty}
\begin{center}
{\bf  Midterm Stat 515}\\ 
{March 4, 2011.}
\end{center}
\begin{flushleft}
Name: %SOLUTIONS
\end{flushleft}

\noindent (1) Please {\it fully} justify your answers and show all your work for full credit.\\
(2) When possible, `box' your final answer to each question (draw a box around it).\\
(3) You may leave your answer unsimplified, for example: $0.7^{4} \frac{18!}{4!8!}, 14 e^{-23}, \sum_{i=1}^n \frac{i}{3^i}$.
\begin{enumerate}
%% Markov chain basics: irred, recurrence, transience, periodicity
%% limiting behavior, stationarity  http://wwwhome.math.utwente.nl/~litvakn/ISP/
\item Suppose the state of a machine at the beginning of each year can
  be described by a random variable in the set $\{0,1,2\}$. 0
  is ``virtually new'', 1 is ``some problems'', and 2 is  ``bad condition''. If
  the machine is in state $i$ at the beginning of the year, the
  probability it will break down that year is $i/2$. If it breaks
  down, it is fixed completely and becomes ``virtually new'' (returns
  to state 0). If it does not break down, its condition becomes worse
  and its state is increased by 1 at the end of the year.
\begin{enumerate}
\item Let $\{X_n,n=0,1,2,\dots\}$ be a stochastic process with $X_n$
  representing the state of the machine at the beginning of the year
  $n$. Find the transition probability matrix of this Markov chain. [2pts]
\item Is the Markov chain irreducible? [1pts]
\item Classify all the states of the Markov chain as either positive
  recurrent, null recurrent or transient. [2pts]
\item What are the periods of the states? [1pts]
\item The cost associated with the machine is (in thousands of
  dollars) 10 for being in state 0, 1 for being in state 1 and 5 for
  being in state 2. What is the long run expected cost of the machine
  ? [2pts]
\item Give the limiting probability that the machine is ``virtually
  new'' {\it and} was in state 1 the year before, that is, find
  $\lim_{n\rightarrow \infty} P(X_{n-1}=1, X_n=0)$. Also clearly state
  the conditions that the Markov chain had to satisfy in order for you
  to solve this problem. [2pts]
\end{enumerate}
{\it Soln:} (a) \begin{equation*}
  P=
  \begin{bmatrix}
    0 & 1 & 0 \\
    1/2 & 0 & 1/2\\
    1 & 0 & 0 \\
  \end{bmatrix},
\end{equation*}
(b) Markov chain is irreducible. (easy to show) \\
(c) Because this is a finite state irreducible Markov chain, every state is positive recurrent.\\
(d) $P_{11}^2 > 0, P_{11}^3 > 0$, and gcd($2,3$) = 1.  Because periodicity is a class property, all states have period =1, hence aperiodic Markov chain.\\
(e)  Solving for  stationary distribution $\pi$ in ergodic equations, we get $\pi = (2/5, 2/5, 1/5)$. Since Markov chain is irreducible and positive recurrent, this gives long term proportion of time the chain spends in each state. Hence, long run cost of the machine is $10\times(2/5) + 1\times(2/5) + 5\times (1/5)= 27/5$. \\
(f) Of course,  since Markov chain is irreducible and ergodic, stationary distribution $\pi$ is also its limiting distribution. We use this here. P($X_{n-1}=1, X_n=0$) =$P(X_n=0\mid X_{n-1}=1) P(X_{n-1}=1) = P_{10} P(X_{n-1}=1)$. Hence, $\lim P(X_{n-1}=1, X_n=0) = P_{10} \pi_1 = (1/2)(2/5) = 1/5.$
\newpage

%% Markov chains: time reversibility, branching processes
\item Consider a Markov chain on a finite state space
  $\Omega=\{1,2,\dots,n\}$. Now assume that the chain has a symmetric
  transition probability matrix $P$ and that $P_{ij}>0$ for all
  $i,j\in \Omega$. Show that this Markov chain has a unique stationary
  distribution $\pi$ that is (discrete) uniform on $\Omega$, and show
  that $\pi$ is also the limiting distribution of the chain. [4pts]\\
  {\it Soln:} Easy to show that the chain is irreducible. Consider $i,j \in \Omega$ with $i< j$. Paths with positive probability: $i\rightarrow i+1 \rightarrow \dots j$, and $j\rightarrow j-1 \rightarrow \dots i$. \\
  Since $P_{ii}>0$ chain is aperiodic. This is a finite state,
  irreducible chain so all states are positive recurrent. Hence, chain
  is irreducible and ergodic so stationary distribution $\pi$ is
  unique and also the limiting distribution of chain. Remains to find
  $\pi$.  Easy to see that detailed balance condition holds for $\pi =
  (1/n,1/n,\dots, 1/n)$, that is $\pi_i P_{ij} = \pi_j P_{ji} \forall
  i,j \in \Omega$ when $\pi = (1/n,1/n,\dots, 1/n)$. Hence the
  discrete uniform distribution on $\Omega$ is the stationary and
  limiting distribution of the Markov chain.

\newpage

%% Poisson process problem http://wwwhome.math.utwente.nl/~litvakn/ISP/
% \item Suppose that print jobs arrive at a network printer with independent and
% exponentially distributed inter-arrival times with parameter 10 per hour. It
% takes the printer exactly 6 seconds to print each page. The sizes of the jobs
% are i.i.d.\ and have a Poisson distribution with a mean of 2 pages.
% \begin{enumerate}
% \item What is the probability that a print job arrives while the previous job is
%    not finished, when this previous job consists of 6 pages? (Assume that the
%    printing of this previous job started immediately after its arrival). [2pts]
% % \item What is the expected arrival time of the first job after 12.00 hours, when
% %    the previous job arrived at 11.58 hours?
% \item Let $M(t)$ be the number of print jobs consisting of more than 3 pages that
%    arrive in a time interval $(0,t]$. What is $P (M (t) = m)$ for $m\in \{0,1,2,\dots \}$? [3pts]
% \end{enumerate}
\item A car mechanic is only able to work on one car at a time.
  Suppose the amount of time he takes to work on each car is an
  exponential random variable with mean 30 minutes, and that these
  times are independent of each other.
\begin{enumerate}
\item Suppose there are always more cars to work on than the mechanic
  has time, and that all customers bring in their cars at 9am. What is
  the expected number of cars the mechanic will have completed work on
  from 9am to 10am ? [2pts]
\item Assume the same set up as in part (a), except now you know that
  the mechanic completed work on exactly 3 cars between 9am and 10am.
  What is the expected value of the time he finished work on the first
  car ? [2pts]
\item Now suppose he schedules two appointments, car A at 9am and car
  B at 9:30am. Assume both cars arrive on time for their appointments.
  Find the expected amount of time that car B spends at the mechanic's
  shop. [3pts]
\end{enumerate}
{\it Soln:} (a) Number of cars mechanic fixes is a Poisson process with $\lambda = 1/30$. Hence, if he works from 9am to 10am (for 60 minutes), the expected number of cars he will have fixed is the expected value of a Poisson r.v. with expectation $\lambda\times 60$ = 2.\\
(b) Let times of completion for the three cars be $X_1,X_2,X_3$. The question is asking what $E(X_1 \mid N(60) = 3)$ is. Now the completion times for these 3 cars is Uniform(0,60) where units are in minutes. Hence $X_1$ is the minimum order statistics of three U($0,60$) random variables. It is easy to derive the cdf of $X_1$ and hence its pdf is $f(x) = \frac{1}{20} (1 - \frac{x}{60})^2,\: x\in (0,60)$  and $E(X_1 \mid N(60) = 3)$ = 15.\\
(c) Condition on whether car A is done by 9:30am or not. Let
completion time for car A be $X_A$. If $X_A < 30$, expected time for
car B to be done is simply 30 minutes.  If $X_A >30$, by
memorylessness of exponential r.v.s, expected additional time for car
A is 30 minutes and time for completion for car B is therefore 30
minutes + 30 minutes (expected value for car B to be done after
mechanic starts working on it.) Hence, expected amount of time car B
spends at the mechanic's shop is:
$$ 30\times P(X_A <30) + (30 +30)\times P(X_A \geq 30) = 30(1-e^{-1}) + 60e^{-1} = 30 + 30e^{-1}. $$

\newpage
%% Iterated expectations/variance (compound Poisson process e.g.) (probl. 87 Ch.5, Ross)
\item Recall that a stochastic process $\{X(t),t \geq 0 \}$ is said to be a compound Poisson process if it can be represented as $X(t)=\sum_{i=1}^{N(t)} Y_i,\: t\geq 0$ where $\{N(t),t\geq 0\}$ is a Poisson process with rate $\lambda$ and $\{Y_i,i \geq 1 \}$ is a family of independent and identically distributed random variables that is also independent of $\{N(t),t\geq 0\}$. Let $\mu_Y=E(Y_i)$  and $\sigma_Y^2 = $Var$(Y_i)$ for $i=1,2,\dots$ 
\begin{enumerate}
%\item Show that this process has independent increments.
\item Find Var($X(t)$). [2pts]
\item Find Cov$(X(t), X(t+s))$, where $t \geq 0,\: s >0$. Hint: it may be useful to use the fact that this process has independent increments. [2pts]
\end{enumerate}
{\it Soln:} (a) Use conditional variance formula, $V(X(t)) = E(V(X(t) \mid N(t)=n)) +  V(E(X(t)) \mid N(t)=n) = \mu_Y^2 \lambda t + \lambda t \sigma_Y^2 $.

(b) To use the independent increments property, rewrite Cov$(X(t), X(t+s))$ = Cov$(X(t), X(t+s)-X(t) + X(t))$ = Cov($X(t), X(t)$) + Cov($X(t), X(t+s)-X(t)$)  = Var($X(t)$) + 0 = answer from part (a).

\end{enumerate}



\end{document}

