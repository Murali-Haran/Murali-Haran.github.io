\documentclass{article}

\setlength{\topmargin}{-.5in}
\setlength{\oddsidemargin}{.0in}
\setlength{\evensidemargin}{.0in}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6.5in}
\setlength{\parindent}{0in}
%\parskip=.125in

\usepackage{amsmath,bm}%
\usepackage{amsfonts}%


\newcommand{\beaa}{\begin{eqnarray*}}
\newcommand{\eeaa}{\end{eqnarray*}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\svskip}{\vspace{.2in}}
\newcommand{\mvskip}{\vspace{.25in}}
\newcommand{\lvskip}{\vspace{.5in}}
\def\E{\mathop{\rm E\,}\nolimits}
\def\Var{\mathop{\rm Var\,}\nolimits}
\def\Cov{\mathop{\rm Cov\,}\nolimits}
\def\Cor{\mathop{\rm Corr\,}\nolimits}
\def\Tr{\mathop{\rm Tr\,}\nolimits}
\def\diag{\mathop{\rm diag\,}\nolimits}
\def\midd{\mathop{\,|\,}\nolimits}
\def\cip{\mathop{\stackrel{P}{\rightarrow}}\nolimits}
\def\cid{\mathop{\stackrel{d}{\rightarrow}}\nolimits}
\def\ciqm{\mathop{\stackrel{\mbox{\scriptsize qm}}{\rightarrow}}\nolimits}
\def\defn{{\stackrel{\mbox{\scriptsize def}}{=}}}
\def\eid{{\stackrel{{\cal D}}{=}}}
\def\rvseq{\mathop{X_1, X_2, \ldots}\nolimits}
\def\rvseqn{\mathop{X_1, \ldots, X_n}\nolimits}
\def\u#1{{\underline{#1}}}
\def\o#1{{\overline{#1}}}
\def\n#1{^{(#1)}}
\newcommand{\qed}{\rule{2mm}{2mm}}

%%%%%%%%%%
% From http://www.disc-conference.org/disc1998/mirror/llncs.sty
\def\vec#1{\mathchoice{\mbox{\boldmath$\displaystyle\bf#1$}}
{\mbox{\boldmath$\textstyle\bf#1$}}
{\mbox{\boldmath$\scriptstyle\bf#1$}}
{\mbox{\boldmath$\scriptscriptstyle\bf#1$}}}
%%%%%%%%%%


\def\cas{\mathop{\stackrel{\mbox{\scriptsize as}}{\rightarrow}}\nolimits}

\pagestyle{empty}

%%-------------------------------------------------------------------

\begin{document}
        \hrule
        \begin{center}
        \Large\bf Stat 515: Stochastic Processes I \hfill Spring 2012\\
        Final Examination  \hfill April 30, 2012
        \end{center}
        \hrule

%\lvskip {\bf Name:  \rule{4in}{.01in}}

\mvskip 
This final exam is worth 30 points.  You have 110 minutes.  
{\bf For full credit, you must explain all of your work!}
Naturally, you may use any results that you know; you should not need to prove anything
unless you are explicitly asked to do so.



\lvskip 
{\bf Problem 1. [14 points]\ } 
Suppose that two telephone operators, Andrew and Barbara, work in an office.
Each one is always either on or off the phone.  Let us assume that each operator's 
time on and off the phone may be modeled by an off-on process with generator (or rate)
matrices as follows:
\[
\mbox{for Andrew:\ }
R_A = \begin{bmatrix} -3 & 3 \\ 3 & -3\end{bmatrix}
\qquad\qquad
\mbox{for Barbara:\ }
R_B = \begin{bmatrix} -1 & 1 \\ 2 & -2\end{bmatrix}
\]
In each matrix, the first row/column is for ``off the phone'' and the second is for ``on the phone.''

% First half:  2 of 6
\svskip
{\bf(a) [2 points]\ }
Suppose that at time zero, Andrew is off the phone.   
Let $N(t)$ equal the total number of transitions (from off to on or on to off) that Andrew makes
before time $t$.  Find the mean and variance of $N(3)$.

% First half:  4 of 6
\svskip
{\bf(b) [2 points]\ }
Let $T_1$ be the time of Andrew's first transition from off to on.  
Conditional on $N(3)=10$, what is the distribution of $T_1$?

% Chapter 6:  2 of 8
\svskip
{\bf(c) [2 points]\ }
Describe how you would find the probability that Barbara is on the phone at time $t=1$ assuming that she is off the phone at $t=0$.
(You do not have to actually make this calculation, but you should describe how to do so.)

% Chapter 6:  4 of 8
\svskip
{\bf(d) [2 points]\ }
Consider the Markov chain with states 1 through 4, as follows:
\[
1: \mbox{A off, B off} \qquad
2: \mbox{A on, B off} \qquad
3: \mbox{A off, B on} \qquad
4: \mbox{A on, B on} \qquad
\]
Assuming that Andrew's process is independent of Barbara's process,
write down the rate matrix $R$ for this four-state process.

% Chapter 6:  6 of 8
\svskip
{\bf(e) [2 points]\ }
Prove that the Markov chain described by the rate matrix $R$ in part (d) 
satisfies detailed balance, i.e., that that the chain is time-reversible.  
(You can prove this even if you do not get the correct
form of $R$.)

% Chapter 6:  8 of 8
\svskip
{\bf(f) [2 points]\ }
Find the stationary distribution $\vec \pi=(\pi_1, \pi_2, \pi_3, \pi_4)^\top$ of the Markov chain
described by the rate matrix in part (d).

% First half:  6 of 6
\svskip
{\bf(g) [2 points]\ }
Let $X_t$, $t=1, 2, \ldots$, be the $t$th state visited by the Markov chain described by the
rate matrix in part (d).  Then the $X_t$ describe a discrete-time, discrete-state-space Markov
chain in which $X_t$ is never equal to $X_{t+1}$.  Derive the probability transition matrix,
$P$, for the $\{X_t\}$ Markov chain and then explain whether the $\{X_t\}$ Markov chain is ergodic.

\svskip
\begin{center}
{\em The exam continues on the other side of this page.}
\end{center}

% Monte Carlo:  2 of 8
\newpage
{\bf Problem 2. [2 points]\ }
Suppose that $X\sim\mbox{gamma}(2.4,2)$, so that $X$ has density function
\[
f(x) = 0.15253x^{1.4}\exp\{-x/2\}, \qquad x>0.
\]
Explain how to use an i.i.d.~sample $Y_1, \ldots, Y_n$ from a standard exponential
distribution to construct 
a 95\% confidence interval for  $P(X>2)$ using importance sampling.
The standard exponential density function is
\[
f(y) = e^{-y}, \qquad y>0.
\]




\lvskip
{\bf Problem 3 [8 points]\ }
Suppose that a joint posterior density for $(\lambda, \theta)$ is
given by
\[
p(\lambda, \theta) \propto
\lambda^2 \theta\exp\{-\theta\lambda - 3\theta - 2\lambda\} .
\]

% MCMC:  3 of 8
\svskip
{\bf(a) [3 points]\ }
Using the fact that a $\mbox{gamma}(\alpha, \beta)$ density has the form
\[
f(x) = \frac{1}{\Gamma(\alpha)\beta^\alpha} x^{\alpha-1} \exp\{-x/\beta\}
\qquad \mbox{for $x>0$},
\]
explain how to implement Gibbs sampling to construct a Markov chain 
with stationary distribution $p(\lambda, \theta)$.

% MCMC:  6 of 8
\svskip
{\bf(b) [3 points]\ }
Explain one way to implement an ``all-at-once'' Metropolis-Hastings algorithm
to construct a Markov chain with stationary distribution $p(\lambda, \theta)$.

% MCMC:  8 of 8
\svskip
{\bf(c) [2 points]\ }
If the Markov chain you obtained in part (a) is $(\lambda_1, \theta_1),
(\lambda_2, \theta_2), \ldots$,
explain how you could find an approximate 95\% confidence interval 
for $E_p(\theta)$.

\lvskip
{\bf Problem 4. [6 points] \ }
We wish to simulate an i.i.d.~sample from the distribution with
cumulative distribution function $F(x) = x^{5/3}$, and therefore
density function $5x^{2/3}/3$, for $0<x<1$.

% Monte Carlo:  5 of 8
\svskip
{\bf(a) [3 points]\ }
Suppose that $U\sim\mbox{uniform}(0,1)$.  Tell how to use the inversion method
to obtain a random variable $X$ with $X\sim F$ {\em and} prove that
your $X$ has the required distribution.

% Monte Carlo:  8 of 8
\svskip
{\bf(b) [3 points]\ }
Suppose $U_1$ and $U_2$ are i.i.d.~$\mbox{uniform}(0,1)$ random variables.
Derive a function $h(x)$ such that conditional on $U_1<h(U_2)$,
$U_2$ has the distribution function $F$.  (This is implementing a rejection method.)
Construct your $h(x)$ so that it gives the smallest possible probability of rejecting
a proposed $X$, and calculate this probability.

\end{document}
