\input{summary}

\begin{document}


\def \thedate{Jan.~11}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\startframe{Reading for next class}
\begin{itemize}
\item 
Read and attempt to understand Sections 2.8 and 3.1 through 3.3 in the Ross book.
\item Continue into Section~3.4 as far as you can (it's 15 pages long).
\end{itemize}
\notes{In the 10th edition, this should be Sections 2.9 and 3.1 through 3.3, then 3.4.}
\end{frame}




\startframe{2.1 Random Variables}
\question{If you roll two 6-sided dice, and you let $X$ equal the total number of dots shown, what is 
$P(X = 4)$?}
{$\frac13$}{$\frac14$}{$\frac16$}{$\frac1{12}$}

\notes{Answer:  D.  Most of the class obtained this answer.
In response to a question, I discussed the reasoning.}

\end{frame}


\startframe{2.2 Discrete Random Variables}
\question{A Bernoulli random variable is a special case of which of the following?}
{binomial random variable}{exponential random variable}{geometric random variable}{Poisson random variable}
\notes{Answer:  A.  This is just terminology, and there were several who did not know
the answer.}
\end{frame}

\startframe{2.2 Discrete Random Variables}
\question{Which of the following is the probability mass function for a Poisson random variable?}
{$p(i) = p(1-p)^i$}
{$p(i) = {n \choose p} p^i(1-p)^{n-i}$}
{$p(i) = \frac{\lambda^i}{i!}e^{-\lambda} $}
{$p(i) = \frac1n$}
\notes{Answer:  C.  We also identified A as geometric, B as binomial, and D as discrete
uniform.}
\end{frame}

\startframe{2.3 ``Continuous'' Random Variables}
\question{If $X$ has a continuous distribution, what is true about its cumulative distribution function $F(x)$?}
{It is strictly increasing.}
{It is everywhere differentiable.}
{It integrates to one.}
{It is continuous.}
\notes{Answer:  D.  This one was slightly tricky and quite a few people answered A.  Also, B
was popular, and some said C because they mixed up cumulative distribution function 
and density function.  We saw for a simple example (a uniform cdf) that a continuous
cdf need be neither strictly increasing nor everywhere differentiable.}
\end{frame}

\startframe{2.3 ``Continuous'' Random Variables}
\question{Which of the following is the probability density function for an exponential random variable?}
{$f(i) = \frac{1}{\beta-\alpha}$}
{$f(i) = \lambda e^{-\lambda x}$}
{$f(i) = \frac{1}{\sigma\sqrt{2\pi}}e^{-(x-\mu)^2/2\sigma^2} $}
{$f(i) = \frac{\lambda e^{-\lambda x} (\lambda x)^{\alpha-1}}{\Gamma(\alpha)}$}
\notes{Answer:  B.  But also notice that D works in the special case $\alpha=1$.  
We identified D as gamma, A as uniform, and C as normal.}
\end{frame}

\startframe{2.4 Expectation of a Random Variable}
\question{If $X$ has a continuous distribution and a density function $f(x)$,  then $E(X)=$?} 
{$\int_{-\infty}^{\infty} \!\! f(x)\,dx$}
{$\int_{-\infty}^{\infty} \!\! xf(x)\,dx$}
{$\int_{-\infty}^{\infty} \!\! x^2f(x)\,dx$}
{$\int_{-\infty}^{\infty} \!\! (x-\mu)f(x)\,dx$}
\notes{Answer: B.  Most people got this one.}
\end{frame}


\startframe{2.4 Expectation of a Random Variable}
\question{The variance is\ldots}
{the expectation of the square.}
{the square of the expectation.}
{the expectation of the square plus the square of the expectation.}
{the expectation of the square minus the square of the expectation.}
\notes{Answer:  D.  Most got this.}
\end{frame}

\startframe{2.5 Jointly Distributed Random Variables}
\question{Which is a simpler way to write $\Var(X) + \Var(Y) + 2\Cov(X,Y)$?}
{$\Var(X+Y)$}
{$\Corr(X,Y)$}
{$E(X+Y)$}
{$E(X+Y)^2$}
\notes{Answer:  A.  A few weren't sure of this.  We didn't spend any time on it, but 
it's possible to derive this formula without too much difficulty.}
\end{frame}

\startframe{2.5 Jointly Distributed Random Variables}
\question{If $X_1, \ldots, X_n$ are independent and identically distributed
with mean $\mu$ and variance $\sigma^2$, 
$\Var \overline X_n = $?}
{$\sigma$}
{$\sigma/n$}
{$(\sigma/n)^2$}
{$\sigma^2/n$}
\notes{Answer:  D.  This is an important one.  For instance, it comes up in the 
proof of the weak law of large numbers using Chebyshev's inequality later.}
\end{frame}

\startframe{2.6 Moment Generating Functions}
\question{If a Bernoulli$(p)$ random variable has MGF $\phi(t)=1-p+pe^t$,
what is the MGF of a binomial$(n,p)$ random variable?}
{$(1-p+pe^t)^n$}
{$n(1-p+pe^t)$}
{$(1-np+npe^t)$}
{$(1-p+pe^t)/n$}
\notes{Answer:  A.  Most knew it.
We discussed moment generating functions for a few minutes.}
\end{frame}

\startframe{2.7 Limit Theorems}
\question{The inequality $P(|X-\mu| \ge k) \le (\sigma/k)^2$ may be used to prove which of the 
following?}
{The Strong Law of Large Numbers}
{The Weak Law of Large Numbers}
{The Central Limit Theorem}
{Chebyshev's Inequality}
\notes{Answer: B.  Few were sure of this.  We spent several minutes with the
students working together to find a proof of the weak law using Chebyshev's inequality.}
\end{frame}

\startframe{2.7 Limit Theorems}
\question{If $X\sim\mbox{binomial}(100,1/2)$ and $\Phi(\cdot)$ is the standard normal cdf, which of the following is the best approximation to $P(X \le 50)$?}
{$\Phi(0)$}
{$\Phi(0.1)$}
{$\Phi(0.2)$}
{$\Phi(0.3)$}
\notes{Answer:  B.  I discussed the continuity correction as it applies here; this continuity correction gives a far more accurate approximation than the more naive choice A.
(The true value is  0.53979,  $\Phi(0)=0.5$, and $\Phi(0.1)=0.53983$.)}

\end{frame}

%\startframe{1.2\quad Properties of Probability}
%
%\begin{columns}
%\begin{column}{4cm}
%
%\tikz{
%\def \lx{-1.4} \def \ux{2.4} \def \ly{-1.2} \def\uy{1.2}
%\draw (\lx,\ly) rectangle (\ux,\uy);
%% grid with dot at origin for helping place objects in rectangle:
%%\draw[step=.5, very thin] (\lx,\ly) grid (\ux,\uy); \filldraw (0,0) circle (.05);
%\draw (0,0) ellipse (1 and .8);
%\draw (1,0) ellipse (1 and .8);
%\draw (0,.9) node {\small$A$};
%\draw (1,.9) node {\small$B$};
%\draw (2.2,1) node{$S$};
%}
%\end{column}
%\begin{column}{6cm}
%Venn diagram depicting events $A$ and $B$.  In the picture, they appear to 
%have a nonempty intersection.
%\end{column}
%\end{columns}
%\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}


