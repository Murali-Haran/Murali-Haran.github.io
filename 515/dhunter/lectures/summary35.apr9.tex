\input{summary}

\begin{document}


\def \thedate{Apr.~9}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\startframe{Announcements}
\begin{itemize}
\item 
All homework must be turned in electronically from now on.
\item 
Only 2 more homeworks!  (Only best 10 out of 11 grades will count.)
\end{itemize}
\end{frame}

\startframe{Monte Carlo methods}
Recall:  Monte Carlo MLE
\begin{itemize}
\item
Given $X\sim f_\theta(x) = \exp\{\theta^\top s(x)\} / c(\theta)$,
write  log-likelihood as
\[
\ell(\theta) = \ell(\theta_0) + (\theta-\theta_0)^\top s(X) - 
\log E_{\theta_0} \exp\{( \theta-\theta_0)^\top s(Y)\}.
\]
\item Use $Y_1, \ldots, Y_m\sim f_{\theta_0}$ to approximate
$\ell(\theta)-\ell(\theta_0)$.
\end{itemize}
\notes{
We discussed this method of Monte Carlo likelihood as it relates to
problem 2 on HW \#10.  In particular, we discussed the fact that
the above framework applies to the case in which $X$ is binomial
(and $s(X)=X$).  This example is presented in Section 3 of
``Improving Simulation-Based Algorithms for Fitting ERGMs''
by Hummel et al (2012), which can be found on my website.
}
\end{frame}

\startframe{Monte Carlo methods}
\begin{itemize}
\item Regular importance sampling:  %With $q(x) = f_{\theta_0}(x)$,
Estimate $E_{f_{\theta_0}} g(X)f_\theta(X)/f_{\theta_0}(X)$ by
\[
\left( \sum_i\frac{g(X_i)r_\theta(X_i)}{r_{\theta_0}(X_i)} \right)
\Bigg/
\left( \sum_i \frac{r_\theta(X_i)}{r_{\theta_0}(X_i)} \right).
\]
\item
In the case of Monte Carlo MLE, $g$ is irrelevant and we only
need the denominator.
\end{itemize}
\notes{This is because the denominator converges almost surely to
$c(\theta)/c(\theta_0)$ by the Strong Law if we take
$r_\theta(x) = \exp\{\theta^\top s(x)\}$.
In other words, Monte Carlo is used here to approximate the
ratio of normalizing constants, which is related to importance
sampling but not exactly the same thing.
}
\end{frame}

\startframe{Markov chain Monte Carlo}
\begin{itemize}
\item Goal:  Estimate $\mu=E_\pi g(X)$ but
cannot %use Monte Carlo directly since we cannot 
sample $X_i\sim \pi$ directly.
\item 
Importance sampling may be difficult, particularly as dimension increases.
\item 
MCMC solution:  Take $\hat\mu = \frac1n\sum g(X_i)$, where
$X_1, X_2, \ldots$ is a simulated Markov chain with stationary distribution $\pi$.
\end{itemize}
\notes{
Here, we are relying on the Strong Law for Markov chains, which says that 
(under some regularity conditions) 
\[
\frac1n \sum_{i=1}^n g(X_i) \stackrel{\rm a.s.}{\to} \mu.
\]
In this case, $\{X_t\}t\ge 0$ is a discrete-time chain, but it can be either
discrete-space or continuous-space.  This is the first time we have considered the
latter case in this class.
}
\end{frame}

\startframe{Markov chain Monte Carlo}
Metropolis-Hastings algorithm (recall HW\#4, problem 5):
\begin{itemize}
\item Start with $X_0=x_0$.  
\item For $i=0, 1, \ldots$, 
Generate $Y \sim q(y\mid x_0)$.
\item Define $\alpha(x,y) = \min\{1, \pi(y)q(x\mid y) / [\pi(x)q(y\mid x)]$.
\item Let $X_{i+1}=Y$ with probability $\alpha(X_i,Y)$.
\end{itemize}
\notes{
There were at least three mistakes on this overhead and in my notes on the chalkboard!
We'll fix all of the mistakes on Wednesday.  (The version above is now correct.)
}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}




