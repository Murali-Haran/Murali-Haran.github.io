\input{summary}

\begin{document}


\def \thedate{Mar.~2}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\startframe{Announcements}
\begin{itemize}
\item 
For Monday, March 12:  Read Sections 6.4, 6.5
\item HW \#7 will not be due until sometime during the week of March 19.
\end{itemize}
\notes{Quick midterm summary:  The median score was 19.5 points out of 20; the mean was 17.4.  
The scores were strongly bimodal.}
\end{frame}

\startframe{6.8 Computing the Transition Probabilities}
The generator (or rate) matrix $R$:
\begin{itemize}
\item Take $r_{ij} = \mbox{rate of $i\to j$ transitions}$, $i\ne j$.
\item Take $r_{ii} = -\sum_{j\ne i} r_{ij}$.
\item This implies $\sum_{j} r_{ij} = \ldots$
\end{itemize}
What is $R$ for the on-off process?  A homogeneous Poisson process?
\notes{This is a slide from an earlier lecture.  For the on-off process, as an example,
we get
\[
R = \begin{bmatrix}
-\lambda_0 & \lambda_0 \\
\lambda_1 & -\lambda_1 
\end{bmatrix}.
\]
}
\end{frame}

\startframe{6.2 Continuous-Time Markov chains}
Recall:
\begin{itemize}
\item $P_{ij}(t) = P[ X(t)= j \mid X(0)=i]$ for all $i$ and $j$.
\item $P(t)$ is the implied transition probability matrix at
time $t$.  
\end{itemize}
\notes{Again, this is review from previous lectures. 
Even for the on-off process, it is not so easy to calculate $P(t)$.
}
\end{frame}

\startframe{6.4 The Transition Probability Function}
Lemma:
\begin{itemize}
\item $\lim_{h\to0} \frac{1-P_{ii}(h)}{h} = -r_{ii}$ \qquad  (in book, $-r_{ii}=v_i$).
\item $\lim_{h\to 0} \frac{P_{ij}(h)}{h} = r_{ij}$ for $i\ne j$
\qquad (in book, $r_{ij}=q_{ij}$).
\end{itemize}
\notes{Proving this fact is not hard; we simply need certain facts regarding Poisson processes
(because the process of leaving the current state is a Poisson process).  Namely,
the probability of one event (starting in state $i$) in a small time $h$ is $\lambda_i h+o(h)$
as $h\to0$.  Also, the probability of two events in a small time is just $o(h)$.
With these facts, the proof merely involves a small bit of algebra.}
\end{frame}

\startframe{6.4 The Transition Probability Function}
\begin{itemize}
\item $P(t)$ is not generally easy to calculate given $R$.
\item We can derive helpful equations as follows:
\item
What is 
\[
\lim_{h\to0} \frac{P_{ij}(t+h) - P_{ij}(t)}{h}?
\]
\end{itemize}
\notes{We derived the answer by rewriting 
$P_{ij}(t+h)$ in two different ways using the Kolmogorov-Chapman equations.
The answer depends on being able to interchange a possibly infinite sum
with the limiting operation $h\to 0$. 
}
\end{frame}

\startframe{6.4 The Transition Probability Function}
Kolmogorov's forward and backward equations:
\begin{itemize}
\item Backward:  $P'(t) = RP(t)$ (always true).
\item Forward:  $P'(t) = P(t)R$ (require ``regularity conditions'').
\end{itemize}
\notes{
These matrix equations merely express the equations for $P'_{ij}(t)$ derived 
using the approach mentioned on the previous slide. 
Notice that this means the $P(t)$ matrix solves one or two differential equations,
which is how we can obtain $P(t)$ if $R$ is known.  In fact, the solution of the
equations is the ``matrix exponential'' 
\[
P(t) =\sum_{n=0}^\infty \frac{(tR)^n}{n!}.
\]
}
\end{frame}

\startframe{6.4 The Transition Probability Function}
From $P'(t) = RP(t)$, plug in $t=0$ to obtain
\[
R = P'(0).
\]
\notes{This is a simple way to obtain $R$ from $P(t)$.}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}


