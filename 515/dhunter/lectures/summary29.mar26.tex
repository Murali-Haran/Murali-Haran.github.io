\input{summary}

\begin{document}


\def \thedate{Mar.~26}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\startframe{Announcements}
\begin{itemize}
\item 
HW \#8 is due on Friday, March 30 at 2:30pm.
\item 
All homework must be turned in electronically from now on.
\item
There is a lot of computing on this assignment; please let me know
early if there will be coding challenges!
\end{itemize}
\notes{We discussed the fact that this course will essentially cover Chapters 1--6 and 11 in
Ross, plus a few additional topics in Monte Carlo methods and MCMC.  Nothing
that is not covered in the course will be on the statistics qualifying review exam.}
\end{frame}


\startframe{Monte Carlo methods}
$\hat\mu = \frac1n \sum_{i=1}^n g(X_i)$, where $X_i$ are i.i.d.~from $f$.
\begin{itemize}
\item Why this works:  (Strong or weak) law of large numbers
\item More precise information is provided by the Central Limit Theorem.
\item The above results are true for vectors $X_i$ of arbitrary 
dimension.
\item {\em Thinking about i.i.d.~Monte Carlo $\equiv$ thinking about basic statistics.}
\end{itemize}
\notes{
We talked a bit more about the CLT, in particular the exact statement of the theorem,
what it means, and how we can use it to give an approximate distribution of the
$\hat\mu$ random variable:
\[
\hat\mu \mbox{\ is approximately\ } N\left(\mu,  \frac{\Var_f g(X)}{n}\right)
\]
}
\end{frame}

\startframe{Monte Carlo methods}
Toy example from Charlie Geyer via Murali Haran:
\begin{itemize}
\item
Given $X_1, X_2 \stackrel{\rm iid}{\sim} N(0,1)$, what is $P(X_2<X_1^2)$?
\item Try numerical integration.
\item Try Monte Carlo approach.
\item Relative advantages / disadvantages?
\end{itemize}
\notes{We went through this example in R, using both the numerical integration and
Monte Carlo approaches.  In this case, numerical integration appears preferable, 
but this may not always be the case, for a couple reasons.  The R code is available
online (see the calendar for today's class).}
\end{frame}

\startframe{Monte Carlo methods}
How do we get ``random'' numbers from an arbitrary distribution $F$?
\begin{itemize}
\item Everything starts with uniform pseudo-random numbers.
\item ``Uniform random'' numbers are neither (!!)
\item Refer to {\tt help(Random)} in R.
\item Generating pseudo-random (and pseudo-i.i.d.) numbers is a huge topic
that we will not explore in depth.
\end{itemize}
\notes{We looked at the {\tt help(Random)} page in R, discussing the
fact that pseudo-random numbers are not at all random but are instead completely deterministic.
However, many very smart people have developed numerous routines over the last several decades
to make the generated numbers appear as ``random'' and ``independent'' as possible.}
\end{frame}

\startframe{Monte Carlo methods}
Properties of uniform random number generators:
\begin{itemize}
\item Repetititve
\item Not independent
\item Not even continuous
\end{itemize}
These are not always disadvantages!
\notes{The period of a random number generator is the number of values that must be simulated before the entire sequence of values repeats itself.  For any good modern algorithm, the period
is much larger than the number of picoseconds in the entire history of the universe!  The non-independence can be an asset because you can set the same seed value before 
generating your random numbers in order to obtain identical results in a repeat of the
experiment.  Finally, the non-continuity results from the fact that computer arithmetic actually divides the unit interval interval into a very large number of tiny increments (about $10^{-15}$ wide), and
only integral multiples of the increment are possible.}
\end{frame}

\startframe{Monte Carlo methods}
How do you generate $X\sim F$ given $U\sim\mbox{Unif}(0,1)$?
\begin{itemize}
\item
Some special cases are covered in Ross, section 11.3
\item
General ``inversion'' method based on  quantile function
\[
F^-(u)  \stackrel{\rm def}{=} \inf \{ x : u\le F(x)\}.
\]
Can prove:  $F^-(U)\sim F$, i.e., $P[F^-(U) \le x] = F(x)$ for all $x$.
%In case $F(x)$ is strictly increasing and continuous, $F^-=F^{-1}$.
\end{itemize}
\notes{A couple important cases to be aware of are the Box-Muller method and the Marsaglia
polar method for generating normal random numbers from uniform random numbers.  
We began discussing the inversion method today and will continue on Wednesday.  Of particular
importance for understanding the intuition of the inversion method is the fact that when 
$F(x)$ is invertible (because it is strictly increasing and continuous), $F^{-1}(u)$ is the
same as $F^-(u)$.  
}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}


