\input{lecture}

\begin{document}


\def \thedate{Apr.~16}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\startframe{Announcements}
\begin{itemize}
\item 
Last homework due Next Monday (Apr.~23)
\item 
Take-home final exam will be available on Saturday, Apr.~21 and due
at 5:00pm on Wednesday, May~2.
\end{itemize}
\end{frame}


\startframe{Markov chain Monte Carlo}
For concise definitions of the conditions for Harris ergodicity\ldots
\begin{itemize}
\item $\pi$-irreducible
\item Aperiodic
\item Harris recurrent
\end{itemize}
\ldots see p.~1711 of Tierney (1994, {\it Annals of Statistics})
\end{frame}

\startframe{Markov chain Monte Carlo}
Strong Law result:
If a Harris-ergodic Markov chain $X_0, X_1, \ldots$ has
stationary distribution $\pi$, and if $\mu=E_\pi g(X)<\infty$, then
\[
\frac1n \sum_{i=1}^n g(X_i) \stackrel{\rm as}{\to} \mu
\quad\mbox{as $n\to\infty$.}
\]
Caution!  These random variables are not i.i.d.!  (Do we have a CLT?)
\end{frame}

\startframe{Markov chain Monte Carlo}
What about burnin?
\begin{itemize}
\item What is it?  
\item Is it necessary?
\end{itemize}
Check out Jones and Hobert (2001, {\it Stat. Sci.})
\end{frame}

\startframe{Markov chain Monte Carlo}
\begin{itemize}
\item ``All at once'' MH:  Go from $\vec X^{(i)} \to \vec X^{(i+1)}$.
\item 
``Variable-at-a-time'' MH:
\[
\begin{bmatrix}
X_1^{(i)} \\ X_2^{(i)} \\ \vdots \\ X_k^{(i)}
\end{bmatrix}
\to
\begin{bmatrix}
X_1^{(i+1)} \\ X_2^{(i)} \\ \vdots \\ X_k^{(i)}
\end{bmatrix}
\to
\begin{bmatrix}
X_1^{(i+1)} \\ X_2^{(i+1)} \\ \vdots \\ X_k^{(i)}
\end{bmatrix}
\to \cdots \to
\begin{bmatrix}
X_1^{(i+1)} \\ X_2^{(i+1)} \\ \vdots \\ X_k^{(i+1)}
\end{bmatrix}
\]
\end{itemize}
\end{frame}

\startframe{Markov chain Monte Carlo}
Some notes about VAATMH:
\begin{itemize}
\item Each $X_k$ might be multivariate.
\item Avoids (or lessens) curse of dimensionality.
\item Need to define ``Full conditional distributions''
\end{itemize}
\end{frame}

\startframe{Markov chain Monte Carlo}
Full conditional distributions
\begin{itemize}
\item Let $\pi(\vec x)$ be the target density.
\item Denote by $\pi(x_k \mid x_{-k})$ the conditional density
of $x_k$ given all of $\vec x$ not including $x_k$.
\item When doing VAATMH, the update to $X_k$ should be 
an ``all-at-once'' MH algorithm with target density $\pi(x_k \mid x_{-k})$.
\end{itemize}
\end{frame}

\startframe{Markov chain Monte Carlo}
Bayesian VAATMH example (from Gilks et al (1996), pp. 75--76):
\begin{itemize}
\item $Y_1, \ldots, Y_n \stackrel{\rm iid}{\sim} N(\mu, \tau^{-1})$
\item $\mu \sim N(0,1)$
\item $\tau \sim \mbox{gamma}(2,1)$
\end{itemize}
Devise an MCMC scheme for sampling from the posterior distribution.
\end{frame}

\startframe{Markov chain Monte Carlo}
Detailed balance, or why M-H works:
\begin{itemize}
\item
\end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}




